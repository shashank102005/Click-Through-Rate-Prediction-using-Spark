{"cells":[{"cell_type":"code","source":["ctr_dataframe = sqlContext.sql(\"SELECT * FROM ctr_df2\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["ctr_dataframe.columns"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["ctr_dataframe.take(5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["ctr_dataframe.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["ctr_dataframe.printSchema"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["############# Collect categorical variable levels \ncategories = {}\nidxCategories = [3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23]\nfor i in idxCategories: ##idxCategories contains indexes of rows that contains categorical data\n    distinctValues = ctr_dataframe.map(lambda x : x[i]).countByValue()\n    categories[i] =  distinctValues\n    \n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["categories[10]"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["########### visualization of the caregorical variables using matplotlib\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import Counter"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["X = np.arange(len(categories[3]))\nf = plt.figure(3)\nplt.bar(X, categories[3].values(), align='center', width=0.5)\nplt.xticks(X, categories[3].keys())\nymax = max(categories[3].values()) + 1\nplt.ylim(0, ymax)\nplt.xlabel('values')\nplt.ylabel('Frequency')\nplt.title('Distribution of Levels for variable: %s'%(ctr_dataframe.columns[3]))\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["X = np.arange(len(categories[4]))\nf = plt.figure(4)\nplt.bar(X, categories[4].values(), align='center', width=0.5)\nplt.xticks(X, categories[4].keys())\nymax = max(categories[4].values()) + 1\nplt.ylim(0, ymax)\nplt.xlabel('values')\nplt.ylabel('Frequency')\nplt.title('Distribution of Levels for variable: %s'%(ctr_dataframe.columns[4]))\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["X = np.arange(len(categories[5]))\nf = plt.figure(5)\nplt.bar(X, categories[5].values(), align='center', width=0.5)\nplt.xticks(X, categories[5].keys())\nymax = max(categories[5].values()) + 1\nplt.ylim(0, ymax)\nplt.xlabel('values')\nplt.ylabel('Frequency')\nplt.title('Distribution of Levels for variable: %s'%(ctr_dataframe.columns[5]))\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["X = np.arange(len(categories[6]))\nf = plt.figure(6)\nplt.bar(X, categories[6].values(), align='center', width=0.5)\nplt.xticks(X, categories[6].keys())\nymax = max(categories[6].values()) + 1\nplt.ylim(0, ymax)\nplt.xlabel('values')\nplt.ylabel('Frequency')\nplt.title('Distribution of Levels for variable: %s'%(ctr_dataframe.columns[6]))\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["############## Use randomSplit with weights and seed to get training, test data sets\nweights = [.8, .2]\nseed = 22\n\nctr_Train, ctr_Test = ctr_dataframe.randomSplit(weights, seed)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["ctr_Train.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["ctr_Test.count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["################### Use StringIndexer and OneHotEncoder to create transformers \nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer,VectorAssembler\n\nC1_Indexer = StringIndexer(inputCol=\"C1\", outputCol=\"indexed_C1\", handleInvalid=\"skip\")\nC1_Encoder = OneHotEncoder(inputCol=\"indexed_C1\", outputCol=\"C1_Vector\")\n\nbanner_pos_Indexer = StringIndexer(inputCol=\"banner_pos\", outputCol=\"indexed_banner_pos\", handleInvalid=\"skip\")\nbanner_pos_Encoder = OneHotEncoder(inputCol=\"indexed_banner_pos\", outputCol=\"banner_pos_Vector\")\n\nsite_id_Indexer = StringIndexer(inputCol=\"site_id\", outputCol=\"indexed_site_id\", handleInvalid=\"skip\")\nsite_id_Encoder = OneHotEncoder(inputCol=\"indexed_site_id\", outputCol=\"site_id_Vector\")\n\nsite_domain_Indexer = StringIndexer(inputCol=\"site_domain\", outputCol=\"indexed_site_domain\", handleInvalid=\"skip\")\nsite_domain_Encoder = OneHotEncoder(inputCol=\"indexed_site_domain\", outputCol=\"site_domain_Vector\")\n\nsite_category_Indexer = StringIndexer(inputCol=\"site_category\", outputCol=\"indexed_site_category\", handleInvalid=\"skip\")\nsite_category_Encoder = OneHotEncoder(inputCol=\"indexed_site_category\", outputCol=\"site_category_Vector\")\n\napp_id_Indexer = StringIndexer(inputCol=\"app_id\", outputCol=\"indexed_app_id\", handleInvalid=\"skip\")\napp_id_Encoder = OneHotEncoder(inputCol=\"indexed_app_id\", outputCol=\"app_id_Vector\")\n\napp_domain_Indexer = StringIndexer(inputCol=\"app_domain\", outputCol=\"indexed_app_domain\", handleInvalid=\"skip\")\napp_domain_Encoder = OneHotEncoder(inputCol=\"indexed_app_domain\", outputCol=\"app_domain_Vector\")\n\napp_category_Indexer = StringIndexer(inputCol=\"app_category\", outputCol=\"indexed_app_category\", handleInvalid=\"skip\")\napp_category_Encoder = OneHotEncoder(inputCol=\"indexed_app_category\", outputCol=\"app_category_Vector\")\n\ndevice_id_Indexer = StringIndexer(inputCol=\"device_id\", outputCol=\"indexed_device_id\", handleInvalid=\"skip\")\ndevice_id_Encoder = OneHotEncoder(inputCol=\"indexed_device_id\", outputCol=\"device_id_Vector\")\n\ndevice_model_Indexer = StringIndexer(inputCol=\"device_model\", outputCol=\"indexed_device_model\", handleInvalid=\"skip\")\ndevice_model_Encoder = OneHotEncoder(inputCol=\"indexed_device_model\", outputCol=\"device_model_Vector\")\n\ndevice_type_Indexer = StringIndexer(inputCol=\"device_type\", outputCol=\"indexed_device_type\", handleInvalid=\"skip\")\ndevice_type_Encoder = OneHotEncoder(inputCol=\"indexed_device_type\", outputCol=\"device_type_Vector\")\n\ndevice_conn_type_Indexer = StringIndexer(inputCol=\"device_conn_type\", outputCol=\"indexed_device_conn_type\", handleInvalid=\"skip\")\ndevice_conn_type_Encoder = OneHotEncoder(inputCol=\"indexed_device_conn_type\", outputCol=\"device_conn_type_Vector\")\n\nC14_Indexer = StringIndexer(inputCol=\"C14\", outputCol=\"indexed_C14\", handleInvalid=\"skip\")\nC14_Encoder = OneHotEncoder(inputCol=\"indexed_C14\", outputCol=\"C14_Vector\")\n\nC15_Indexer = StringIndexer(inputCol=\"C15\", outputCol=\"indexed_C15\", handleInvalid=\"skip\")\nC15_Encoder = OneHotEncoder(inputCol=\"indexed_C15\", outputCol=\"C15_Vector\")\n\nC16_Indexer = StringIndexer(inputCol=\"C16\", outputCol=\"indexed_C16\", handleInvalid=\"skip\")\nC16_Encoder = OneHotEncoder(inputCol=\"indexed_C16\", outputCol=\"C16_Vector\")\n\nC17_Indexer = StringIndexer(inputCol=\"C17\", outputCol=\"indexed_C17\", handleInvalid=\"skip\")\nC17_Encoder = OneHotEncoder(inputCol=\"indexed_C17\", outputCol=\"C17_Vector\")\n\nC18_Indexer = StringIndexer(inputCol=\"C18\", outputCol=\"indexed_C18\", handleInvalid=\"skip\")\nC18_Encoder = OneHotEncoder(inputCol=\"indexed_C18\", outputCol=\"C18_Vector\")\n\nC19_Indexer = StringIndexer(inputCol=\"C19\", outputCol=\"indexed_C19\", handleInvalid=\"skip\")\nC19_Encoder = OneHotEncoder(inputCol=\"indexed_C19\", outputCol=\"C19_Vector\")\n\nC20_Indexer = StringIndexer(inputCol=\"C20\", outputCol=\"indexed_C20\", handleInvalid=\"skip\")\nC20_Encoder = OneHotEncoder(inputCol=\"indexed_C20\", outputCol=\"C20_Vector\")\n\nC21_Indexer = StringIndexer(inputCol=\"C21\", outputCol=\"indexed_C21\", handleInvalid=\"skip\")\nC21_Encoder = OneHotEncoder(inputCol=\"indexed_C21\", outputCol=\"C21_Vector\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["########## Create the assembler stage for the pipeline \nfrom pyspark.ml import Pipeline\nfeat_Assembler = VectorAssembler(\n    inputCols=[\"C1_Vector\",\"banner_pos_Vector\",\"site_id_Vector\",\"site_domain_Vector\",\"site_category_Vector\",\"app_id_Vector\",\n              \"app_domain_Vector\",\"app_category_Vector\",\"device_id_Vector\",\"device_model_Vector\",\"device_type_Vector\",\n              \"device_conn_type_Vector\",\"C14_Vector\",\"C15_Vector\",\"C16_Vector\",\"C17_Vector\",\"C18_Vector\",\"C19_Vector\",\n              \"C20_Vector\",\"C21_Vector\",\"hour\"],\n    outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["############### Define the Logistic Regression stage for the pipeline \nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=150, regParam=0.01)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["################## Construct the machine learning pipeline \nfrom pyspark.ml import Pipeline\n    \npipeline = Pipeline(stages=[C1_Indexer,banner_pos_Indexer,site_id_Indexer,site_domain_Indexer,site_category_Indexer,app_id_Indexer,\n                     app_domain_Indexer,app_category_Indexer,device_id_Indexer,device_model_Indexer,device_type_Indexer,\n                     device_conn_type_Indexer,C14_Indexer,C15_Indexer,C16_Indexer,C17_Indexer,C18_Indexer,C19_Indexer,C20_Indexer,\n                     C21_Indexer,C1_Encoder,banner_pos_Encoder,site_id_Encoder,site_domain_Encoder,site_category_Encoder,\n                     app_id_Encoder,app_domain_Encoder,app_category_Encoder,device_id_Encoder,device_model_Encoder,device_type_Encoder,\n                     device_conn_type_Encoder,C14_Encoder,C15_Encoder,C16_Encoder,C17_Encoder,C18_Encoder,C19_Encoder,C20_Encoder,\n                     C21_Encoder,feat_Assembler, lr])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["############### Create a pipeline model to train\nmodel = pipeline.fit(ctr_Train)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["########### Transform the test data set to produce predictions and compare predictions to labels to determine accuracy of the model\noutput = model.transform(ctr_Test).select(\"features\", \"label\", \"prediction\", \"rawPrediction\", \"probability\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["Prediction = output.select(\"prediction\",\"label\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["Prediction.take(10)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["accuracy = Prediction.filter(Prediction['label'] == Prediction['prediction']).count() / float(Prediction.count())\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["accuracy"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from math import log\n############Calculates the value of log loss for a given probabilty and label.\ndef computeLogLoss(p, y):\n    epsilon = 1e-15\n    if y == 1:\n        return -log(epsilon + p) if p == 0 else -log(p)\n    elif y == 0:\n        return -log(1 - p + epsilon) if p == 1 else -log(1 - p)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["Predict_Prob = output.select(\"label\",\"probability\",\"prediction\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["Predict_Prob.map(lambda l: (l.label,l.probability[1])).take(5)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["Predict_Prob.take(5)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["####Calculates the log loss for the data given the model.\ndef evaluateResults(Predictions, Test_Data):\n    return Predictions.map(lambda l: computeLogLoss(l.probability[1],l.label)).sum() / Test_Data.count()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["############ the log loss of the model\nprint(evaluateResults(Predict_Prob,ctr_Test))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"ctr_Prediction","notebookId":1697723769175584},"nbformat":4,"nbformat_minor":0}
